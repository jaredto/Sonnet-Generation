{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395a0aab",
   "metadata": {},
   "source": [
    "# Homework 4: Perceptrons and Deep Learning [25 pts]\n",
    "## Comp562 Summer II 2023\n",
    "\n",
    "### Due 11:59pm July 25, 2023\n",
    "\n",
    "In this assignment, you will use the perceptron and deep learning models discussed in class and experiment with some toy data. To avoid unexpected behavior with cached variables, always test your code with a fresh kernel. For hardware acceleration, use Colab with GPU enabled.\n",
    "\n",
    "### Problem 1: Perceptron and MLP Model [5 pts]\n",
    "**(1a)** Compare and contrast the perceptron model with SVM and linear regression. [3 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d431956",
   "metadata": {},
   "source": [
    "The perceptron model is a basic neural network while SVM is a classification algorithm. They both find a hyperplane to seperate linearly seperable data, but perceptrons use iterative optimization while SVM maximizes the margin between support vectors. SVMs can handle non-linearly serperable data with the kernal trick, while perceptrons will not converge and you will need to force termination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5881f",
   "metadata": {},
   "source": [
    "**(1b)** What is the purpose of nonlinear activations? [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91de076",
   "metadata": {},
   "source": [
    "Without nonlinear activations, the entire network is just a linear transformation, and could only learn linear relationships in the data. The nonlinear activations make it able to represent more complex relationships by introducing nonlinearity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c4064",
   "metadata": {},
   "source": [
    "### Problem 2: LSTM [10 pts]\n",
    "**(2a)** Why are long-term dependencies hard to model with standard recurrent neural networks? [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d918584",
   "metadata": {},
   "source": [
    "The main problems are the gradient vanishing, where when the gradients go to zero, or the gradient exploding, where it goes to infinity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d6cd9",
   "metadata": {},
   "source": [
    "**(2b)** Submit your modified sonnet generation code in a separate Gradescope assignment [4 pts]. (Completion, no answer required here.)\n",
    "\n",
    "The following questions relate to your sonnet generation model.\n",
    "\n",
    "**(2c)** Justify your choice of model architectures and the changes you made to the model architecture from the provided code. [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de58d8c7",
   "metadata": {},
   "source": [
    "I increased the hidden layers to find more complex dependencies in the data, and increased the layers to add more recurrency, making it hopefully more coherent. I also introduced a scheduler to change the learning rate during training as I thought it would achieve a better convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1274730",
   "metadata": {},
   "source": [
    "**(2d)** Jusify any other changes you made to the sonnet generation code. [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1f408",
   "metadata": {},
   "source": [
    "I increased the epochs to 1000 to train for a much longer time to further reduce the error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c66bbe",
   "metadata": {},
   "source": [
    "**(2e)** Analyze the sample sonnets generated from your model. Are they realistic? Given unlimited computing power, what modifications would you make to improve their quality? [1 pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a21cc0",
   "metadata": {},
   "source": [
    "The sonnets are not actually that bad, besides the fact that some of the words are not real the grammar and flow is actually pretty good, and are actually coherent. I would modify the code so that hopefully there would not be any fake words, maybe changing the randomness of the char generations, and with unlimited computing power, make the network deeper and train on many more epochs. This is assuming I had enough training data to not cause overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03222b",
   "metadata": {},
   "source": [
    "### Problem 3: Convolutional Neural Networks [10 pts]\n",
    "\n",
    "**(3a)** Load the torchvision [CIFAR10 dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10). Design and train a model to perform 10-class classification on this dataset. Implementations that use models loaded from torch hub will not recieve full credit. Your model should improve over a baseline accuracy of 40%. [6 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512) \n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "      # DO NOT CALL THIS FUNCTION DIRECTLY\n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# instantiate model on selected device\n",
    "classifier = MyModel().to(device)\n",
    "print(classifier)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# since this is a multi-class classification problem, cross entropy is appropriate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimize with stochastic gradient descent, setting learning rate and momentum\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=0.5, std=0.5)])\n",
    "\n",
    "# define batch size\n",
    "batch_size = 4\n",
    "\n",
    "# download data\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "# wrap in iterable dataloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "  # trainloader yields batched data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # put on GPU if applicable\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = classifier(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    avg_train_loss = running_loss / len(trainloader)\n",
    "    train_loss.append(avg_train_loss)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        running_val_loss = 0.0\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = classifier(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(testloader)\n",
    "        val_loss.append(avg_val_loss)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9feeb5",
   "metadata": {},
   "source": [
    "**(3b)** Plot your training and validation losses. (Hint: provided code demonstrates usage of labels in plt plotting. You will likely want to modify the plotting function to scale validation appropriately.) [1 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: track losses for plotting\n",
    "#added above\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(val_loss, label='validation / 24')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1caf7",
   "metadata": {},
   "source": [
    "**(3c)** Evaluate your model on the test partition. [1 pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = classifier(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on the test partition: {(100 * correct / total):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35072ea2",
   "metadata": {},
   "source": [
    "**(3d)** Justify your choice of model architecture. [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c971eca",
   "metadata": {},
   "source": [
    "I used a convolutional neural network as they are great for image classification problems. The CNN layers detects patterns which helps the CNN recognize more complex patterns, which is useful for image classification. The CNN also recognizes features regardless of its position in the image, and it can use pooling. Overall, CNNs are known to be great at image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d36d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
